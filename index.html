<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>JS Example</title>
    <style>
      .action-buttons {
        display: flex;
        margin: 10px;
        gap: 10px;
      }
      .conversation-container {
        display: flex;
        flex-direction: column;
        gap: 10px;
        padding: 10px;
        width: calc(100vw - 40px);
        max-height: 70vh;
        overflow-y: scroll;
      }

      .conversation-container div {
        padding: 10px;
        border-radius: 5px;
        background-color: #f1f1f1;
        width: fit-content;
      }
    </style>
  </head>
  <body>
    <h1>Screen Audio and Microphone Audio</h1>
    <div class="action-buttons">
      <button id="setToken">Fetch Token</button>
      <select id="language">
        <option value="en" selected>English</option>
        <option value="hi">Hindi</option>
      </select>
      <button id="startButton">Start</button>
      <button id="stopButton" disabled>Stop</button>
      <button id="resetButton">Reset</button>
    </div>

    <div class="conversation-container" id="chat-container"></div>

    <script>
      const bearerToken = "LPDehf0IKXH1EzQhV6Io6B8bfs68VzmQ";
      let url = "wss://eu2.rt.speechmatics.com/v2?jwt=";
      const nullValues = ["", null, undefined];

      let speakerSource;
      let microphoneSource;
      let jwt = "";
      let session;
      let session2;
      let mediaRecorder;
      let mediaRecorder2;
      let seq = 0;
      let seq2 = 0;
      let lastMessageFrom = "";
      let userLastChatBubbleId = 0;
      let agentLastChatBubbleId = 0;

      let message = {
        message: "StartRecognition",
        audio_format: {
          type: "file",
        },
        transcription_config: {
          language: "en",
          diarization: "speaker",
          operating_point: "enhanced",
          // enable_partials: true,
          max_delay: 1,
        },
      };

      function displayMessage(type, text, bubbleId) {
        if (!nullValues.includes(text) && !nullValues.includes(text?.trim())) {
          const bubbleSpan = document.getElementById(
            `${type}-bubble-${bubbleId}`
          );
          if (!bubbleSpan) {
            const div = document.createElement("div");
            div.innerHTML = `<strong>${type}: </strong><span id = "${type}-bubble-${bubbleId}">${text}</span>`;
            document.getElementById("chat-container").appendChild(div);
          } else {
            bubbleSpan.textContent += text;
          }
          // Scroll to bottom
          document.getElementById("chat-container").scrollTop =
            document.getElementById("chat-container").scrollHeight;
        }
      }

      function handleMessage(type, text) {
        let chatBubbleId =
          type === "User" ? userLastChatBubbleId : agentLastChatBubbleId;

        if (lastMessageFrom === "") {
          lastMessageFrom = type;
          displayMessage(type, text, chatBubbleId);
        } else {
          if (lastMessageFrom === type) {
            displayMessage(type, text, chatBubbleId);
          } else {
            displayMessage(type, text, chatBubbleId);
            lastMessageFrom = type;
            if (type === "User") {
              agentLastChatBubbleId++;
            } else {
              userLastChatBubbleId++;
            }
          }
        }
      }

      document.getElementById("resetButton").onclick = function () {
        document.getElementById("chat-container").innerHTML = "";
      };

      document.getElementById("language").onchange = function (event) {
        const language = event.target.value;
        message.transcription_config.language = language;
      };

      document.getElementById("setToken").onclick = async function () {
        const response = await fetch(
          "https://mp.speechmatics.com/v1/api_keys?type=rt",
          {
            method: "POST",
            headers: {
              "Content-Type": "application/json",
              Authorization: "Bearer " + bearerToken,
            },
            body: JSON.stringify({ ttl: 3600 }),
          }
        );
        const data = await response.json();
        jwt = data["key_value"];

        alert("Token fetched successfully.");
      };

      async function captureAudio() {
        document.getElementById("startButton").onclick = async function () {
          document.getElementById("startButton").disabled = true;
          document.getElementById("stopButton").disabled = false;
          document.getElementById("language").disabled = true;

          session = new WebSocket(url + jwt);

          session.onopen = async () => {
            session.send(JSON.stringify(message));
            try {
              const screenStream = await navigator.mediaDevices.getDisplayMedia(
                {
                  audio: true,
                }
              );

              let speakerAudioContext = new AudioContext();
              let speakerDestination =
                speakerAudioContext.createMediaStreamDestination();

              speakerSource =
                speakerAudioContext.createMediaStreamSource(screenStream);
              speakerSource.connect(speakerDestination);

              mediaRecorder = new MediaRecorder(speakerDestination.stream, {
                mimeType: "audio/webm",
              });

              mediaRecorder.start(1000);

              mediaRecorder.ondataavailable = async (event) => {
                if (event.data.size > 0) {
                  session.send(event.data);
                }
              };
            } catch (err) {
              console.error("Error capturing audio: ", err);
            }
          };
          session.onmessage = (event) => {
            const data = JSON.parse(event.data);

            if (data && data.message === "AudioAdded") {
              seq = data.seq_no;
            }
            if (data && data.message === "AddTranscript") {
              const conversationText = data.metadata.transcript;
              if (
                !nullValues.includes(conversationText) &&
                !["", "."].includes(conversationText.trim())
              ) {
                handleMessage("User", conversationText);
              }
            }

            if (data && data.message === "EndOfStream") {
              session.close();
            }
          };

          session2 = new WebSocket(url + jwt);
          session2.onopen = async () => {
            session2.send(JSON.stringify(message));
            let micStream = await navigator.mediaDevices.getUserMedia({
              audio: true,
            });

            let options = {};
            if (MediaRecorder.isTypeSupported("audio/webm")) {
              options = {
                mimeType: "audio/webm",
              };
            } else if (MediaRecorder.isTypeSupported("audio/mp4")) {
              options = {
                mimeType: "audio/mp4",
              };
            } else {
              console.error("no suitable mimetype found for this device");
            }

            let micAudioContext = new AudioContext();
            let micDestination = micAudioContext.createMediaStreamDestination();

            microphoneSource =
              micAudioContext.createMediaStreamSource(micStream);
            microphoneSource.connect(micDestination);

            mediaRecorder2 = new MediaRecorder(micStream, options);
            mediaRecorder2.start(1000);

            mediaRecorder2.ondataavailable = async (event) => {
              if (event.data.size > 0) {
                session2.send(event.data);
              }
            };
          };

          session2.onmessage = (event) => {
            const data = JSON.parse(event.data);

            if (data && data.message === "AudioAdded") {
              seq2 = data.seq_no;
            }

            if (data && data.message === "AddTranscript") {
              const conversationText = data.metadata.transcript;
              if (
                !nullValues.includes(conversationText) &&
                !["", "."].includes(conversationText.trim())
              ) {
                handleMessage("Agent", conversationText);
              }
            }

            if (data && data.message === "EndOfStream") {
              session2.close();
            }
          };
        };

        document.getElementById("stopButton").onclick = function () {
          if (mediaRecorder) {
            mediaRecorder.stop();
            mediaRecorder.stream.getTracks().forEach((track) => track.stop());
            mediaRecorder = null;

            speakerSource?.disconnect();
            speakerSource?.mediaStream
              ?.getTracks()
              ?.forEach((track) => track?.stop());
            speakerSource = null;
          }
          if (mediaRecorder2) {
            mediaRecorder2.stop();
            mediaRecorder2.stream.getTracks().forEach((track) => track.stop());
            mediaRecorder2 = null;

            microphoneSource?.disconnect();
            microphoneSource?.mediaStream
              ?.getTracks()
              ?.forEach((track) => track?.stop());
            microphoneSource = null;
          }

          if (session) {
            const message = {
              message: "EndOfStream",
              last_seq_no: parseInt(seq, 10),
            };
            session.send(JSON.stringify(message));
          }
          if (session2) {
            const message = {
              message: "EndOfStream",
              last_seq_no: parseInt(seq2, 10),
            };
            session2.send(JSON.stringify(message));
          }

          document.getElementById("startButton").disabled = false;
          document.getElementById("stopButton").disabled = true;
          document.getElementById("language").disabled = false;

          console.log("Recording stopped.");
        };
      }
      captureAudio();
    </script>
  </body>
</html>
